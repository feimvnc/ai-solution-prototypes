{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acf836e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Users/user/opt/anaconda3/envs/gpt-local/lib/python3.11/site-packages (1.7.4)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install deepface\n",
    "# !pip install faiss-cpu\n",
    "# restart jupyter notebook, if lib is not found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1be3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from deepface import DeepFace     # import lib\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d305fd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "representations= []\n",
    "for root, _dir, files in os.walk('deepface/tests/dataset/'):    # image datasets to create embeddings\n",
    "    for file in files:\n",
    "        if \".jpg\" in file:\n",
    "            exact_file = f'{root}{file}'\n",
    "            objs = DeepFace.represent(\n",
    "                img_path=exact_file,\n",
    "                model_name=\"Facenet\",\n",
    "                detector_backend=\"mtcnn\"\n",
    "            )\n",
    "            for obj in objs:\n",
    "                embedding = obj[\"embedding\"]\n",
    "                representations.append([file, embedding])\n",
    "            \n",
    "#             print(exact_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf62454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check counts, we have 71 seed images\n",
    "len(representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "117440a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check embedding dimensions, 128, we will use to create synthetic images \n",
    "len(representations[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d54f216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 1000000 (one million) synthetic data, and their embeddings\n",
    "for i in range(len(representations), 1000000):\n",
    "    key = f\"synthetic_{i}.jpg\"\n",
    "    vector = [random.gauss(-0.5, 0.5) for z in range(128)]\n",
    "    representations.append([key, vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d2bedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(representations)       # see the length of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7e0b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings \n",
    "embeddings = []\n",
    "for key, value in representations:\n",
    "    embeddings.append(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee3257",
   "metadata": {},
   "source": [
    "## Initialize faiss index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f57b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidean \n",
    "index = faiss.IndexFlatL2(128)      # use dimension of 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93d3f1c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# faiss index requires to pass np array\n",
    "index.add(np.array(embeddings, dtype = \"f\"))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86136fdc",
   "metadata": {},
   "source": [
    "## compare target image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "56ba93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"target/target.jpg\"    # given an unseen target image, to find in the trained dataset/embedddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c2f411b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "# We use same models used for embedding for target \n",
    "target_embedding = DeepFace.represent(\n",
    "    img_path = target_path,\n",
    "    model_name = \"Facenet\",\n",
    "    detector_backend = \"mtcnn\"\n",
    ")[0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "50f329ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embedding = np.array(target_embedding, dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8d40d2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2cc8fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embedding = np.expand_dims(target_embedding, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b23049ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020e485",
   "metadata": {},
   "source": [
    "## search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "97d34569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 3 closest neighbors \n",
    "import time\n",
    "k = 3 \n",
    "tic = time.time()\n",
    "distances, neighbors = index.search(target_embedding, k)\n",
    "toc = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "612be014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03563284873962402 seconds\n"
     ]
    }
   ],
   "source": [
    "print(toc - tic, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "87668dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41, 45, 68]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find image of k nearest neighbors\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "38c75590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('img5.jpg', 'img6.jpg', 'img10.jpg')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the image names \n",
    "representations[41][0], representations[45][0], representations[68][0], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dc20c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open above 3 images, you should see dataset images match target image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
